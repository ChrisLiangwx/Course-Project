ytest = test.set$Sales
bart.fit = gbart(xtrain, ytrain, x.test = xtest)
bart.pred = bart.fit$yhat.test.mean
bart.mse = mean((ytest - bart.pred)^2)
bart.mse
library(tree)
library(ISLR2)
library(randomForest)
library(caTools)
library(BART)
library(gbm)
attach(Hitters)
head(Hitters)
Hitters = na.omit(Hitters)
Hitters$Salary = log(Hitters$Salary)
#prepare test and train set
set.seed(1)
sample.data = sample.split(Hitters$Salary, SplitRatio = 200/nrow(Hitters))
train.set = subset(Hitters, sample.data)
test.set = subset(Hitters, !sample.data)
#boosting
shrinkage_values = seq(0.01, 0.1, by = 0.01)
train.mse = numeric(length(shrinkage_values))
test.mse = numeric(length(shrinkage_values))
for (i in seq_along(shrinkage_values)) {
set.seed(1)
boost.fit = gbm(Salary ~ ., data = train.set, distribution = "gaussian",
n.trees = 1000, interaction.depth = 1,
shrinkage = shrinkage_values[i])
train.pred = predict(boost.fit, train.set, n.trees = 1000)
test.pred = predict(boost.fit, test.set, n.trees = 1000)
train.mse[i] = mean((train.set$Salary - train.pred)^2)
test.mse[i] = mean((test.set$Salary - test.pred)^2)
}
#plot
plot(shrinkage_values, train.mse, type = "b", col = "blue",
xlab = "Shrinkage Values", ylab = "Training MSE", main = "Training MSE vs. Shrinkage")
plot(shrinkage_values, test.mse, type = "b", col = "red",
xlab = "Shrinkage Values", ylab = "Test MSE", main = "Test MSE vs. Shrinkage")
summary(boost.fit)
lm.fit = lm(Salary~., data = train.set)
lm.pred = predict(lm.fit, newdata = test.set)
lm.mse = mean((test.set$Salary - lm.pred)^2)
lm.mse
summary(lm.fit)
lm.fit2 = lm(Salary~. + Years:Walks, data = train.set)
lm.pred2 = predict(lm.fit2, newdata = test.set)
lm.mse2 = mean((test.set$Salary - lm.pred2)^2)
lm.mse2
set.seed(1)
bag.fit = randomForest(Salary ~ ., data = train.set, mtry = ncol(train.set) - 1,
ntree = 500)
bag.pred = predict(bag.fit, test.set)
bag.mse = mean((test.set$Salary - bag.pred)^2)
bag.mse
library(tree)
library(ISLR2)
library(randomForest)
library(caTools)
library(BART)
library(gbm)
data(package = 'ISLR2')
head(Bikeshare)
attach(Bikeshare)
set.seed(1)
#tree
sample.data = sample.split(Bikeshare$bikers, SplitRatio = 0.6)
train.set = subset(Bikeshare, sample.data)
test.set = subset(Bikeshare, !sample.data)
tree.fit = tree(bikers ~ ., data = train.set)
tree.pred = predict(tree.fit, newdata = test.set)
tree.mse = mean((test.set$bikers - tree.pred)^2)
tree.mse
shrinkage_values = seq(0.01, 0.1, by = 0.01)
test.mse = numeric(length(shrinkage_values))
for (i in seq_along(shrinkage_values)) {
set.seed(1)
boost.fit = gbm(bikers ~ ., data = train.set, distribution = "gaussian",
n.trees = 1000, interaction.depth = 1,
shrinkage = shrinkage_values[i])
train.pred = predict(boost.fit, train.set, n.trees = 1000)
test.pred = predict(boost.fit, test.set, n.trees = 1000)
test.mse[i] = mean((test.set$bikers - test.pred)^2)
}
test.mse
bagging.fit = randomForest(bikers ~ ., data = train.set, mtry = 5)
bagging.pred = predict(bagging.fit, test.set)
bagging.mse = mean((test.set$bikers - bagging.pred)^2)
bagging.mse
rf.fit = randomForest(bikers ~ ., data = train.set)
rf.pred = predict(rf.fit, test.set)
rf.mse = mean((test.set$bikers - rf.pred)^2)
rf.mse
xtrain = train.set[, -which(names(train.set) == "bikers")]
ytrain = train.set$bikers
xtest = test.set[, -which(names(test.set) == "bikers")]
ytest = test.set$bikers
bart.fit = gbart(xtrain, ytrain, x.test = xtest)
bart.pred = bart.fit$yhat.test.mean
bart.mse = mean((ytest - bart.pred)^2)
bart.mse
lm.fit = lm(bikers~., data = train.set)
lm.pred = predict(lm.fit, newdata = test.set)
lm.mse = mean((test.set$bikers - lm.pred)^2)
lm.mse
library(tree)
library(ISLR2)
library(randomForest)
library(caTools)
library(BART)
attach(OJ)
head(OJ)
#prepare test and train set
set.seed(1)
sample.data = sample.split(OJ$Purchase, SplitRatio = 800/nrow(OJ))
train.set = subset(OJ, sample.data)
test.set = subset(OJ, !sample.data)
#fit tree
tree.fit = tree(Purchase ~ ., data = train.set)
summary(tree.fit)
plot(tree.fit)
text(tree.fit)
#predict
tree.preds = predict(tree.fit, newdata = test.set, type = "class")
table(tree.preds, test.set$Purchase)
(22+25) / (22+25+140+83)
#cv
cv.tree.fit = cv.tree(tree.fit)
plot(cv.tree.fit$size, cv.tree.fit$dev, type = "b")
opt.size = which.min(cv.tree.fit$dev)
pruned.tree = prune.tree(tree.fit, best = cv.tree.fit$size[opt.size])
plot(pruned.tree)
text(pruned.tree, pretty = 0)
#test.set
pruned.preds = predict(pruned.tree, newdata = test.set, type = "class")
table(pruned.preds, test.set$Purchase)
(25+22)/(140+83+25+22)
#train.set
pruned.preds = predict(pruned.tree, newdata = train.set, type = "class")
table(pruned.preds, train.set$Purchase)
(59+72)/(429+240+59+72)
library(ISLR2)
library(e1071)
library(caTools)
set.seed(1)
x = matrix(rnorm(100 * 2), ncol = 2)
y = c(rep(-1, 60), rep(1, 40))
x[1:30, ] = x[1:30, ] + 3
x[31:60, ] = x[31:60, ] -3
data = data.frame(x=x, y=as.factor(y))
plot(x, col = y + 2)
sample.data = sample.split(data$x.1, SplitRatio = 0.6)
train.set = subset(data, sample.data)
test.set = subset(data, !sample.data)
#polynomial kernel
set.seed(1)
tune.out = tune(svm, y~., data = train.set, kernel = 'polynomial',
ranges = list(degree = 1:5, cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
bestmod = tune.out$best.model
plot(bestmod, data)
ypred = predict(bestmod, test.set)
table(predict = ypred, truth = test.set$y)
#radial kernel
set.seed(1)
tune.out = tune(svm, y~., data = train.set, kernel = 'radial',
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
bestmod = tune.out$best.model
plot(bestmod, data)
ypred = predict(bestmod, test.set)
table(predict = ypred, truth = test.set$y)
library(ISLR2)
library(e1071)
library(caTools)
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1*(x1^2-x2^2>0)
data = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
plot(x1, x2, col = y + 1)
#logistic regression
lr.fit = glm(y ~ x1 + x2, data = data, family = 'binomial')
lr.probs = predict(lr.fit, newdata = data, type = 'response')
lr.preds = rep(0,500)
lr.preds[lr.probs >0.5] = 1
table(lr.preds, data$y)
plot(x1, x2, col = lr.preds + 1)
#lr non-linear
lr.fit2 = glm(y~I(x1^2) + I(x2^2), data = data, family = 'binomial')
#lr non-linear
lr.fit2 = glm(y~I(x1^2) + I(x2^2), data = data, family = 'binomial')
lr.probs2 = predict(lr.fit2, newdata = data, type = 'response')
lr.preds2 = rep(0,500)
lr.preds2[lr.probs2 >0.5] = 1
table(lr.preds2, data$y)
plot(x1, x2, col = lr.preds2 + 1)
plot(x1, x2, col = y + 1)
library(ISLR2)
library(e1071)
library(caTools)
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1*(x1^2-x2^2>0)
data = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
plot(x1, x2, col = y + 1)
set.seed(1)
tune.out = tune(svm, y~x1+x2, data = data, kernel = 'linear',
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
bestmod = tune.out$best.model
plot(bestmod, data)
#SVM non-linear kernel
set.seed(1)
tune.out = tune(svm, y~x1+x2, data = data, kernel = 'polynomial',
ranges = list(degree = 1:2, cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
bestmod = tune.out$best.model
plot(bestmod, data)
library(ISLR2)
library(e1071)
library(caTools)
#load Auto
setwd("E:\\Projects\\Course Project\\APPL DATA ANALYTICS\\M5")
Auto = read.table("Auto.data", header=T, na.strings = c("?", " ", "NA"), stringsAsFactors = T)
Auto = na.omit(Auto)
attach(Auto)
set.seed(1)
head(Auto)
mpg.median = median(mpg)
Auto$mpg_binary = ifelse(Auto$mpg > mpg.median, 1, 0)
head(Auto)
tune.linear = tune(svm, mpg_binary~.-mpg, data = Auto, kernel = 'linear',
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.linear)
bestmod = tune.linear$best.model
plot(bestmod, Auto, horsepower ~ weight)
svm_model_2d = svm(mpg_binary ~ weight + horsepower, data = Auto, kernel = "linear", cost = 1)
grid_range = expand.grid(weight = seq(min(Auto$weight), max(Auto$weight), length.out = 100),
horsepower = seq(min(Auto$horsepower), max(Auto$horsepower), length.out = 100))
grid_range$mpg_binary = predict(svm_model_2d, newdata = grid_range)
plot(Auto$weight, Auto$horsepower, col = Auto$mpg_binary + 1, xlab = "Weight", ylab = "Horsepower", pch = 19)
tune.nonlinear = tune(svm, mpg_binary~.-mpg, data = Auto, kernel = 'radial',
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma = c(0.5, 1, 1.5, 2, 2.5, 3)))
bestmod = tune.linear$best.model
summary(tune.nonlinear)
bestmod
plot(bestmod, Auto, horsepower ~ weight)
tune.nonlinear = tune(svm, mpg_binary~.-mpg, data = Auto, kernel = 'polynomial',
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree = 1:5))
bestmod = tune.linear$best.model
summary(tune.nonlinear)
bestmod$degree
library(ISLR2)
library(MASS)
library(caTools)
set.seed(1)
head(Default)
attach(Default)
#SplitRatio = 0.6
sample.data = sample.split(Default, SplitRatio = 0.6)
train.set = subset(Default, sample.data)
test.set = subset(Default, !sample.data)
lr.fit = glm(default ~ income + balance, data = train.set, family = binomial)
lr.probs = predict(lr.fit, test.set, type = "response")
lr.preds = rep("No", nrow(test.set))
lr.preds[lr.probs > 0.5] = "Yes"
table(lr.preds, test.set$default)
(23+100)/(4827+50+100+23)
#add student to model
lr.fit1 = glm(default ~ income + balance + student, data = train.set, family = binomial)
lr.probs1 = predict(lr.fit, test.set, type = "response")
lr.preds1 = rep("No", nrow(test.set))
lr.preds1[lr.probs1 > 0.5] = "Yes"
table(lr.preds1, test.set$default)
library(ISLR2)
library(MASS)
library(caTools)
library(boot)
set.seed(1)
head(Default)
attach(Default)
lr.fit = glm(default ~ income + balance, data = train.set, family = binomial)
summary(lr.fit)$coefficient
boot.fn = function(data, index){
lr.fit = glm(default~income + balance, data = Default, subset = index, family = binomial)
return(coef(lr.fit))
}
boot.fn(Default, 1:nrow(Default))
boot.fit = boot(Default, boot.fn, R = 1000)
boot.fit
boot.fit
library(ISLR2)
library(MASS)
library(caTools)
library(boot)
# set.seed(2)
set.seed(1)
x = rnorm(100)
y = x-2*x^2+rnorm(100)
plot(x,y)
model_i = glm(y ~ x, data = data)
library(ISLR2)
library(MASS)
library(caTools)
library(boot)
# set.seed(2)
set.seed(1)
x = rnorm(100)
y = x-2*x^2+rnorm(100)
plot(x,y)
#model with different degrees
data = data.frame(x,y)
#i
model_i = glm(y ~ x, data = data)
cv_i = cv.glm(data, model_i)
cv_error_i = cv_i$delta[1]
#ii
data$x2 = x^2
model_ii = glm(y ~ x + x2, data = data)
cv_ii = cv.glm(data, model_ii)
data$x3 = x^3
model_iii = glm(y ~ x + x2 + x3, data = data)
cv_iii = cv.glm(data, model_iii)
cv_error_iii = cv_iii$delta[1]
#iv
data$x4 = x^4
model_iv = glm(y ~ x + x2 + x3 + x4, data = data)
cv_iv = cv.glm(data, model_iv)
cv_error_iv = cv_iv$delta[1]
cv_error_i
cv_error_ii
library(ISLR2)
library(MASS)
library(caTools)
library(boot)
# set.seed(2)
set.seed(1)
x = rnorm(100)
y = x-2*x^2+rnorm(100)
plot(x,y)
#model with different degrees
data = data.frame(x,y)
#i
model_i = glm(y ~ x, data = data)
cv_i = cv.glm(data, model_i)
cv_error_i = cv_i$delta[1]
#ii
data$x2 = x^2
model_ii = glm(y ~ x + x2, data = data)
cv_ii = cv.glm(data, model_ii)
cv_error_ii = cv_ii$delta[1]
#iii
data$x3 = x^3
model_iii = glm(y ~ x + x2 + x3, data = data)
cv_iii = cv.glm(data, model_iii)
cv_error_iii = cv_iii$delta[1]
#iv
data$x4 = x^4
model_iv = glm(y ~ x + x2 + x3 + x4, data = data)
cv_iv = cv.glm(data, model_iv)
cv_error_iv = cv_iv$delta[1]
cv_error_i
cv_error_ii
cv_error_iii
cv_error_iv
summary(model_i)
summary(model_ii)
summary(model_iii)
summary(model_iv)
scaled.data = scale(USArrests)
cor.dis = as.dist((1 - cor(t(scaled.data)))/2)
euc.dis = dist(scaled.data)
ratio = cor.dis / (euc.dis^2)
mean(ratio)
sd(ratio)
set.seed(1)
method.complete = hclust(dist(USArrests), method = 'complete')
plot(method.complete)
method.complete.cut = cutree(method.complete, k = 3)
method.complete.cut
scaled.data = scale(USArrests)
method.complete.scaled = hclust(dist(scaled.data), method = 'complete')
plot(method.complete.scaled)
par(mfrow = c(1, 2))
plot(method.complete, main = "Complete Linkage without Scaling")
plot(method.complete.scaled, main = "Complete Linkage with Scaling")
set.seed(1)
class1 = matrix(rnorm(20*50, mean = 0), ncol = 50)
class2 = matrix(rnorm(20*50, mean = 1), ncol = 50)
class3 = matrix(rnorm(20*50, mean = 2), ncol = 50)
data = rbind(class1, class2, class3)
dim(data)
pca = prcomp(data)
plot(pca$x[,1:2], col = rep(1:3, each = 20), pch = 19, xlab = "PC1", ylab = "PC2")
#k-means k = 3
km = kmeans(data, centers = 3, nstart = 20)
table(km$cluster, rep(1:3, each = 20))
#k-means k = 2
km2 = kmeans(data, centers = 2, nstart = 20)
table(km2$cluster, rep(1:3, each = 20))
#k-means k = 4
km3 = kmeans(data, centers = 4, nstart = 20)
table(km3$cluster, rep(1:3, each = 20))
km.pca = kmeans(pca$x[,1:2], centers = 3, nstart = 20)
table(km.pca$cluster, rep(1:3, each = 20))
#scaling
scaled.data = scale(data)
km.scaled = kmeans(scaled_data, centers = 3, nstart = 20)
#scaling
scaled.data = scale(data)
km.scaled = kmeans(scaled_data, centers = 3, nstart = 20)
#scaling
scaled.data = scale(data)
set.seed(1)
class1 = matrix(rnorm(20*50, mean = 0), ncol = 50)
class2 = matrix(rnorm(20*50, mean = 1), ncol = 50)
class3 = matrix(rnorm(20*50, mean = 2), ncol = 50)
data = rbind(class1, class2, class3)
dim(data)
#scaling
scaled.data = scale(data)
km.scaled = kmeans(scaled.data, centers = 3, nstart = 20)
table(km.scaled$cluster, rep(1:3, each = 20))
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
train_index = sample(1:nrow(Boston), 0.7 * nrow(Boston))
boston.test = Boston[-train_index, ]
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
boston.train = Boston[train_index, ]
boston.test = Boston[-train_index, ]
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
sample.data = sample.split(Boston$medv, SplitRatio = 0.7)
train = subset(sample.data)
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
sample.data = sample.split(Boston$medv, SplitRatio = 0.7)
train = subset(Boston, sample.data)
test = subset(Boston, !sample.data)
boston.test = Boston[-train_index, ]
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
sample.data = sample.split(Boston$medv, SplitRatio = 0.7)
train = subset(Boston, sample.data)
test = subset(Boston, !sample.data)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
source("E:/Projects/Course Project/APPL DATA ANALYTICS/M4/Applied-7.R", echo=TRUE)
lirbary(caTools)
train = sample(1:nrow(Boston), nrow(Boston) / 2)
test = Boston$medv[-train]
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston) / 2)
test = Boston$medv[-train]
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
library(tree)
library(ISLR2)
library(randomForest)
head(Boston)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston) / 2)
test = Boston$medv[-train]
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 16, ntree = 500, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 500, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, ntree = 500, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, ntree = 25, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, ntree = 5000, importance = TRUE)
bag.boston
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, test)
abline(0, 1)
mean((yhat.bag - test)^2)
